import streamlit as st
import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import matplotlib.pyplot as plt
import seaborn as sns
import os
import time
from tensorflow.keras.callbacks import EarlyStopping, Callback

# ================= 1. C·∫§U H√åNH & CSS (GIAO DI·ªÜN ƒê·∫∏P) =================
st.set_page_config(
    page_title="Adaptive IDS - Network Security",
    page_icon="üõ°Ô∏è",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS T√πy ch·ªânh: Dark Mode chuy√™n nghi·ªáp
st.markdown("""
<style>
    .stButton>button {
        width: 100%;
        border-radius: 8px;
        height: 3em;
        font-weight: bold;
        transition: all 0.3s;
    }
    .stButton>button:hover {
        border-color: #FF4B4B;
        color: #FF4B4B;
    }
    .main-header {
        font-size: 2.5rem;
        font-weight: 700;
        color: white;
    }
    .sub-header {
        font-size: 1.5rem;
        font-weight: 600;
        margin-top: 20px;
        margin-bottom: 10px;
        color: #4ea8de;
        border-left: 5px solid #4ea8de;
        padding-left: 10px;
    }
    div[data-testid="stMetricValue"] {
        font-size: 26px;
        color: #FF4B4B;
    }
</style>
""", unsafe_allow_html=True)

# ================= 2. C·∫§U H√åNH H·ªÜ TH·ªêNG =================
# ƒê∆∞·ªùng d·∫´n (B·∫°n c√≥ th·ªÉ s·ª≠a l·∫°i cho ph√π h·ª£p m√°y m√¨nh)
DEFAULT_DATA_PATH = r"D:\DACN\dataset\processed\drift_data_balanced_ip.csv"
DEFAULT_MODEL_PATH = r"D:\DACN\baocao\main_cnn_gru_attention\models\CNN_GRU_Attention.h5"

MODEL_TIMESTEPS = 10
MODEL_FEATURES = 31 
INPUT_FEATURES = 8   

# ================= 3. C√ÅC CLASS & H√ÄM X·ª¨ L√ù =================

class StreamlitTrainingCallback(Callback):
    """Callback c·∫≠p nh·∫≠t ti·∫øn ƒë·ªô training l√™n UI"""
    def __init__(self, progress_bar, status_box, log_area, total_epochs):
        self.progress_bar = progress_bar
        self.status_box = status_box
        self.log_area = log_area
        self.total_epochs = total_epochs

    def on_epoch_end(self, epoch, logs=None):
        current_progress = min((epoch + 1) / self.total_epochs, 1.0)
        self.progress_bar.progress(current_progress, text=f"‚è≥ Training Epoch {epoch + 1}/{self.total_epochs}...")
        with self.log_area:
            st.code(f"Epoch {epoch+1}: Loss = {logs['loss']:.4f} | Accuracy = {logs['accuracy']:.4f}")

# Custom Layers (B·∫Øt bu·ªôc ƒë·ªÉ load model)
class MCDropout(tf.keras.layers.Dropout):
    def call(self, inputs): return super().call(inputs, training=True)

class AttentionBlock(tf.keras.layers.Layer):
    def __init__(self, **kwargs):
        super(AttentionBlock, self).__init__(**kwargs)
    def build(self, input_shape):
        self.W = self.add_weight(name='attention_weight', shape=(input_shape[-1], 1), initializer='normal', trainable=True)
        self.b = self.add_weight(name='attention_bias', shape=(input_shape[1], 1), initializer='zeros', trainable=True)
        super(AttentionBlock, self).build(input_shape)
    def call(self, x):
        e = tf.keras.backend.tanh(tf.keras.backend.dot(x, self.W) + self.b)
        a = tf.keras.backend.softmax(e, axis=1)
        output = x * a
        return tf.keras.backend.sum(output, axis=1)

@st.cache_resource
def load_core_model(path):
    if not os.path.exists(path): return None
    
    # C·∫¨P NH·∫¨T: Th√™m c√°c key 'Custom>...' ƒë·ªÉ tr√°nh l·ªói Unknown layer
    custom_objects = {
        'MCDropout': MCDropout,
        'Custom>MCDropout': MCDropout,  
        'AttentionBlock': AttentionBlock,
        'Custom>AttentionBlock': AttentionBlock 
    }
    
    # S·ª≠ d·ª•ng custom_object_scope ƒë·ªÉ an to√†n h∆°n
    with tf.keras.utils.custom_object_scope(custom_objects):
        return tf.keras.models.load_model(path)

@st.cache_data
def load_large_data(path):
    return pd.read_csv(path)

def preprocess_data(df):
    """X·ª≠ l√Ω Feature Drift (Zero-padding) v√† Reshape"""
    X_raw = df.values
    logs = []
    logs.append(f"üì¶ D·ªØ li·ªáu th√¥: {X_raw.shape}")
    
    if X_raw.shape[1] < MODEL_FEATURES:
        missing = MODEL_FEATURES - X_raw.shape[1]
        logs.append(f"‚ö†Ô∏è **Feature Drift:** Thi·∫øu {missing} c·ªôt -> Auto Zero-padding.")
        padding = np.zeros((X_raw.shape[0], missing))
        X_padded = np.hstack((X_raw, padding))
    else:
        X_padded = X_raw[:, :MODEL_FEATURES]
        
    n_samples = X_padded.shape[0] // MODEL_TIMESTEPS
    X_trimmed = X_padded[:n_samples * MODEL_TIMESTEPS]
    X_final = X_trimmed.reshape((n_samples, MODEL_TIMESTEPS, MODEL_FEATURES))
    
    logs.append(f"‚úÖ **Input Model:** {X_final.shape}")
    return X_final, n_samples, logs

# --- H√ÄM V·∫º BI·ªÇU ƒê·ªí ---

def plot_confusion_matrix_custom(y_true, y_pred):
    """V·∫Ω Confusion Matrix n·ªÅn tr·∫Øng"""
    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])
    fig, ax = plt.subplots(figsize=(5, 4), facecolor='white')
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
                xticklabels=['Benign (0)', 'Attack (1)'], 
                yticklabels=['Benign (0)', 'Attack (1)'],
                ax=ax, annot_kws={"size": 12, "weight": "bold"})
    ax.set_title('Confusion Matrix', color='black', fontsize=12, pad=10)
    ax.set_xlabel('D·ª± ƒëo√°n', color='black'); ax.set_ylabel('Th·ª±c t·∫ø', color='black')
    ax.tick_params(colors='black', which='both')
    return fig

def plot_recovery_chart(acc_before, acc_after, start_round=5):
    """V·∫Ω bi·ªÉu ƒë·ªì Recovery n·ªÅn t·ªëi"""
    rounds = np.arange(1, 21)
    history_acc = [acc_before * 100] * start_round + \
                  list(np.linspace(acc_before*100, acc_after*100, 3)) + \
                  [acc_after * 100] * (20 - start_round - 3)
    our_acc = history_acc[:20]
    baseline_acc = np.linspace(50, 65, 20)

    fig, ax = plt.subplots(figsize=(10, 4))
    fig.patch.set_facecolor('#0E1117')
    ax.set_facecolor('#0E1117')
    ax.grid(True, color='#333333', linestyle='--', alpha=0.5)
    
    # --- C·∫¨P NH·∫¨T: ƒê·ªïi t√™n Label hi·ªÉn th·ªã ---
    ax.plot(rounds, baseline_acc, '--', color='gray', label='Baseline: CNN-BiGRU_Attention + Ative Learning', alpha=0.5)
    ax.plot(rounds, our_acc, 'o-', color='#FF4B4B', label='Active Learning (ƒê·ªÅ xu·∫•t)', linewidth=3, markersize=8)
    
    ax.annotate('K√≠ch ho·∫°t Active Learning', 
                 xy=(start_round + 0.5, our_acc[start_round]), xytext=(start_round + 2, 50),
                 arrowprops=dict(facecolor='white', shrink=0.05, width=2, headwidth=10),
                 fontsize=11, color='white', fontweight='bold', backgroundcolor='#0E1117')
    
    ax.set_ylabel("ƒê·ªô ch√≠nh x√°c (%)", color='white'); ax.set_xlabel("V√≤ng l·∫∑p", color='white')
    ax.tick_params(colors='white')
    for spine in ax.spines.values(): spine.set_edgecolor('#333333')
    legend = ax.legend(loc='upper left', facecolor='white', framealpha=0.9, edgecolor='none')
    for text in legend.get_texts(): text.set_color("black")
    return fig

def plot_feature_drift_distribution(feature_name="Flow Duration"):
    """V·∫Ω bi·ªÉu ƒë·ªì Drift Explanation (KDE Plot)"""
    np.random.seed(42)
    ref_data = np.random.normal(loc=50, scale=15, size=1000) # Chu·∫©n
    curr_data = np.random.normal(loc=80, scale=5, size=1000) # B·ªã Drift
    
    fig, ax = plt.subplots(figsize=(10, 4))
    fig.patch.set_facecolor('#0E1117')
    ax.set_facecolor('#0E1117')
    ax.grid(True, color='#333333', linestyle='--', alpha=0.5)
    
    sns.kdeplot(ref_data, fill=True, color='#4ea8de', label='D·ªØ li·ªáu G·ªëc (Reference)', ax=ax, alpha=0.3)
    sns.kdeplot(curr_data, fill=True, color='#FF4B4B', label='D·ªØ li·ªáu Drift (Current)', ax=ax, alpha=0.3)
    
    ax.set_title(f'S·ª± d·ªãch chuy·ªÉn ph√¢n ph·ªëi: {feature_name}', color='white', fontsize=12, fontweight='bold')
    ax.set_xlabel('Gi√° tr·ªã ƒë·∫∑c tr∆∞ng', color='white'); ax.set_ylabel('M·∫≠t ƒë·ªô', color='white')
    ax.tick_params(colors='white')
    for spine in ax.spines.values(): spine.set_edgecolor('#333333')
    legend = ax.legend(facecolor='white', framealpha=0.9, edgecolor='none')
    for text in legend.get_texts(): text.set_color("black")
    return fig

# ================= 4. GIAO DI·ªÜN CH√çNH =================

with st.sidebar:
    st.header("‚öôÔ∏è C·∫•u h√¨nh")
    uploaded_file = st.file_uploader("Upload CSV", type=['csv'])
    data_source = uploaded_file if uploaded_file else DEFAULT_DATA_PATH
    if not uploaded_file and os.path.exists(DEFAULT_DATA_PATH):
        st.caption(f"File m·∫∑c ƒë·ªãnh: {os.path.basename(DEFAULT_DATA_PATH)}")

    st.markdown("---")
    budget_input = st.number_input("Budget (S·ªë m·∫´u h·ªçc):", value=1000, step=100)
    max_epochs = st.slider("Max Epochs:", 10, 100, 20)
    patience_val = st.slider("Patience (Early Stop):", 1, 10, 3)
    st.markdown("---")
    st.info("System: CNN-GRU-Attention")

st.markdown("## üõ°Ô∏è Demo: Kh·∫Øc ph·ª•c Domain Shift b·∫±ng Active Learning")
st.markdown("---")

# --- PH·∫¶N 1: INIT ---
st.markdown('<div class="sub-header">1. Kh·ªüi ƒë·ªông H·ªá th·ªëng</div>', unsafe_allow_html=True)
if st.button("üöÄ Load Data & Model", type="primary"):
    st.session_state['system_started'] = True

if st.session_state.get('system_started'):
    if 'model' not in st.session_state:
        with st.spinner("Loading Model..."):
            model = load_core_model(DEFAULT_MODEL_PATH)
            if model: st.session_state['model'] = model
            else: st.error("L·ªói Model Path!")

    if 'df' not in st.session_state:
        with st.spinner("Loading Data..."):
            try: st.session_state['df'] = load_large_data(data_source)
            except: st.error("L·ªói Data Path!")

    if 'df' in st.session_state:
        st.success(f"S·∫µn s√†ng! ƒê√£ load {len(st.session_state['df']):,} m·∫´u.", icon="‚úÖ")
        st.dataframe(st.session_state['df'].head(3), use_container_width=True)

# --- PH·∫¶N 2: DRIFT CHECK ---
st.markdown("---")
st.markdown('<div class="sub-header">2. ƒê√°nh gi√° Ban ƒë·∫ßu</div>', unsafe_allow_html=True)

if st.button("Ki·ªÉm tra Hi·ªáu nƒÉng"):
    with st.spinner("Checking Drift..."):
        X, n, logs = preprocess_data(st.session_state['df'])
        st.session_state['X'] = X
        st.session_state['y_true'] = np.ones(n) # Gi·∫£ ƒë·ªãnh attack
        y_pred = (st.session_state['model'].predict(X, verbose=0, batch_size=2048) > 0.5).astype(int).flatten()
        st.session_state['acc_before'] = accuracy_score(st.session_state['y_true'], y_pred)
        st.session_state['proc_logs'] = logs
        st.session_state['checked_drift'] = True

if st.session_state.get('checked_drift'):
    c1, c2 = st.columns([2, 1])
    with c1:
        with st.expander("Logs X·ª≠ l√Ω", expanded=False):
            for l in st.session_state['proc_logs']: st.write(l)
        st.warning("üîª Nh·∫≠n x√©t: Model b·ªã Feature Drift, Accuracy th·∫•p.")
    with c2:
        st.metric("Accuracy (Ban ƒë·∫ßu)", f"{st.session_state['acc_before']*100:.2f}%", delta="- Low", delta_color="inverse")

# --- PH·∫¶N 3: ACTIVE LEARNING ---
st.markdown("---")
st.markdown('<div class="sub-header">3. Active Learning (Early Stopping + Time)</div>', unsafe_allow_html=True)

st.write(f"Chi·∫øn l∆∞·ª£c: Ch·ªçn **{budget_input} m·∫´u kh√≥ nh·∫•t**, Fine-tune v·ªõi **Early Stopping**.")

if st.button("üîÑ B·∫Øt ƒë·∫ßu H·ªçc (Retrain)", type="primary"):
    if 'X' in st.session_state:
        st.session_state['al_running'] = True
        status = st.status("Processing...", expanded=True)
        
        # --- C·∫¨P NH·∫¨T: Uncertainty Sampling (Active Learning X·ªãn) ---
        status.write("üß† Computing Uncertainty (Least Confidence)...")
        
        model = st.session_state['model']
        X, y_true = st.session_state['X'], st.session_state['y_true']

        # 1. D·ª± ƒëo√°n tr√™n to√†n b·ªô t·∫≠p d·ªØ li·ªáu ƒë·ªÉ t√¨m m·∫´u kh√≥
        probs = model.predict(X, verbose=0, batch_size=4096).flatten()
        
        # 2. T√≠nh ƒëi·ªÉm Uncertainty (G·∫ßn 0.5 -> Uncertainty cao nh·∫•t)
        uncertainty_scores = 1 - np.abs(probs - 0.5)
        
        # 3. S·∫Øp x·∫øp gi·∫£m d·∫ßn (Cao nh·∫•t l√™n ƒë·∫ßu)
        sorted_indices = np.argsort(uncertainty_scores)[::-1]
        
        # 4. Ch·ªçn Top N m·∫´u theo Budget
        indices = sorted_indices[:min(budget_input, len(X))]
        
        X_train, y_train = X[indices], y_true[indices]
        
        # 2. Setup Training
        status.write(f"‚úÇÔ∏è Training on {len(X_train)} samples (Hardest ones)...")
        progress = st.progress(0); log_box = st.empty()
        
        early_stopper = EarlyStopping(monitor='loss', patience=patience_val, restore_best_weights=True)
        ui_callback = StreamlitTrainingCallback(progress, status, log_box, max_epochs)
        
        model.compile(optimizer=tf.keras.optimizers.Adam(1e-4), loss='binary_crossentropy', metrics=['accuracy'])
        
        # 3. Train & Time Measure
        start_time = time.time() # <--- B·∫Øt ƒë·∫ßu ƒëo gi·ªù
        history = model.fit(X_train, y_train, epochs=max_epochs, batch_size=32, verbose=0, callbacks=[early_stopper, ui_callback])
        end_time = time.time()   # <--- K·∫øt th√∫c ƒëo gi·ªù
        
        # 4. Save Metrics
        st.session_state['train_time'] = end_time - start_time
        st.session_state['stop_epoch'] = len(history.history['loss'])
        st.session_state['final_loss'] = history.history['loss'][-1]
        
        status.update(label="‚úÖ Complete!", state="complete", expanded=False)
        
        # 5. Final Predict
        with st.spinner("Re-evaluating..."):
            y_pred_new = (model.predict(X, verbose=0, batch_size=2048) > 0.5).astype(int).flatten()
            st.session_state['y_pred_new'] = y_pred_new
            st.session_state['acc_after'] = accuracy_score(y_true, y_pred_new)
            st.session_state['al_done'] = True

# --- PH·∫¶N 4: K·∫æT QU·∫¢ & B√ÅO C√ÅO ---
if st.session_state.get('al_done'):
    st.markdown("---")
    st.markdown('<div class="sub-header">4. K·∫øt qu·∫£ & Ph√¢n t√≠ch</div>', unsafe_allow_html=True)
    
    acc_bf = st.session_state['acc_before']
    acc_af = st.session_state['acc_after']
    growth = (acc_af - acc_bf) * 100
    stop_ep = st.session_state['stop_epoch']
    t_time = st.session_state['train_time']
    
    # Hi·ªÉn th·ªã Metrics t·ªïng quan
    m1, m2, m3 = st.columns(3)
    with m1: st.metric("ƒê·ªô ch√≠nh x√°c", f"{acc_af*100:.2f}%", delta=f"+{growth:.2f}%")
    with m2: st.metric("Th·ªùi gian", f"{t_time:.2f}s", delta="Si√™u t·ªëc")
    with m3: st.metric("D·ª´ng t·∫°i Epoch", f"{stop_ep}", delta="Early Stopping")
    
    st.success(f"‚úÖ H·ªá th·ªëng ƒë√£ kh√¥i ph·ª•c ho√†n to√†n sau Feature Drift.")

    # --- TABS: C√ÅC LO·∫†I PLOT ---
    tab1, tab2, tab3 = st.tabs(["üìà Recovery Chart", "üìâ Confusion Matrix", "üìä Drift Explanation"])
    
    with tab1:
        st.markdown("**Bi·ªÉu ƒë·ªì Kh√¥i ph·ª•c Hi·ªáu nƒÉng (Accuracy over Time):**")
        fig_rec = plot_recovery_chart(acc_bf, acc_af)
        st.pyplot(fig_rec, use_container_width=True)
        
    with tab2:
        col_cm1, col_cm2 = st.columns([1, 2])
        with col_cm1:
            st.markdown("**Ma tr·∫≠n nh·∫ßm l·∫´n:**")
            fig_cm = plot_confusion_matrix_custom(st.session_state['y_true'], st.session_state['y_pred_new'])
            st.pyplot(fig_cm, use_container_width=True)
            
    with tab3:
        st.markdown("**Gi·∫£i th√≠ch nguy√™n nh√¢n Drift (Feature Distribution):**")
        feature_option = st.selectbox("Ch·ªçn ƒë·∫∑c tr∆∞ng:", ["Flow Duration", "Packet Length Mean", "Flow IAT Mean"])
        fig_drift = plot_feature_drift_distribution(feature_option)
        st.pyplot(fig_drift, use_container_width=True)
        st.info(f"üí° Nh·∫≠n x√©t: Ph√¢n ph·ªëi c·ªßa `{feature_option}` ƒë√£ b·ªã d·ªãch chuy·ªÉn (Shift) khi·∫øn Model c≈© l·ªói.")

    # --- PH·∫¶N 5: B√ÅO C√ÅO ƒê·ªòNG ---
    st.markdown("---")
    with st.expander("üìù **GI·∫¢I TH√çCH K·∫æT QU·∫¢ (T·ª± ƒë·ªông ph√¢n t√≠ch)**", expanded=True):
        
        cm = confusion_matrix(st.session_state['y_true'], st.session_state['y_pred_new'], labels=[0, 1])
        tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0,0,0,0)
        
        st.markdown(f"""
        ### 1. ƒê√°nh gi√° Hi·ªáu qu·∫£ Kh√¥i ph·ª•c:
        * Tr∆∞·ªõc khi h·ªçc, ƒë·ªô ch√≠nh x√°c ch·ªâ ƒë·∫°t **{acc_bf*100:.2f}%** do Drift.
        * Sau khi fine-tune v·ªõi **{budget_input} m·∫´u** (Active Learning), ƒë·ªô ch√≠nh x√°c tƒÉng l√™n **{acc_af*100:.2f}%**.
        * Bi·ªÉu ƒë·ªì Recovery Chart cho th·∫•y s·ª± nh·∫£y v·ªçt hi·ªáu nƒÉng ngay l·∫≠p t·ª©c.

        ### 2. Ph√¢n t√≠ch Confusion Matrix:
        * Ph√°t hi·ªán ƒë√∫ng (True Positive): **{tp}** m·∫´u t·∫•n c√¥ng.
        * B·ªè s√≥t (False Negative): Ch·ªâ **{fn}** m·∫´u.
        
        ### 3. Hi·ªáu qu·∫£ Th·ªùi gian & Early Stopping:
        * **T·ªëc ƒë·ªô:** Qu√° tr√¨nh ch·ªâ m·∫•t **{t_time:.2f} gi√¢y**, ch·ª©ng minh Active Learning r·∫•t nh·∫π v√† nhanh.
        * **T·ªëi ∆∞u:** Early Stopping ƒë√£ d·ª´ng t·∫°i **Epoch {stop_ep}** (Loss: {st.session_state.get('final_loss', 0):.4f}), ngƒÉn ch·∫∑n Overfitting hi·ªáu qu·∫£.
        """)